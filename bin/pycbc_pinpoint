#!/usr/bin/env python
# Copyright (C) 2020 Alex Nitz
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

import logging, argparse, numpy, argparse, pycbc

import scipy.special
import numpy.random

from pycbc import vetoes, psd, waveform, events, strain, detector, filter, DYN_RANGE_FAC
from pycbc.types import TimeSeries, zeros, float32, complex64

parser = argparse.ArgumentParser(usage='',
    description="Targetted Coherent Analysis for a Precisely Known Sky Location")
parser.add_argument("-V", "--verbose", action="store_true",
                  help="print extra debugging information", default=False)
parser.add_argument("--output-file", type=str)
parser.add_argument("--bank-file", type=str)
parser.add_argument("--instruments", nargs='*', type=str)
parser.add_argument("--low-frequency-cutoff", type=float,
                  help="The low frequency cutoff to use for filtering (Hz)")
parser.add_argument("--chisq-bins", default=0)
parser.add_argument("--chisq-threshold", type=float, default=0)
parser.add_argument("--right-ascension", type=float)
parser.add_argument("--declination", type=float)
parser.add_argument("--network-snr-threshold", type=float, default=5.0, help="""
                    Triggers with network snr below this value will
                    be discarded.""")
parser.add_argument("--cluster-window", type=float, default=1.0,
                  help="Length of clustering window in seconds.")
parser.add_argument("--marg-seed", type=int, default=0,
                  help="Seed to choose points to sample likelihood")
parser.add_argument("--marg-size", type=int, default=100000,
                  help="Number of samples to take from likelihood")

# Add options groups
strain.insert_strain_option_group_multi_ifo(parser)
strain.StrainSegments.insert_segment_option_group_multi_ifo(parser)
psd.insert_psd_option_group_multi_ifo(parser)
args = parser.parse_args()

ifos = args.instruments
flow = args.low_frequency_cutoff

# Option verifications
strain.verify_strain_options_multi_ifo(args, parser, ifos)
strain.StrainSegments.verify_segment_options_multi_ifo(args, parser,
                                                       ifos)
psd.verify_psd_options_multi_ifo(args, parser, ifos)
pycbc.init_logging(args.verbose)


strain_dict = strain.from_cli_multi_ifos(args, ifos,
                                         dyn_range_fac=DYN_RANGE_FAC
                                        )
strain_segments_dict = strain.StrainSegments.from_cli_multi_ifos(
                                             args, strain_dict, ifos)

logging.info("Making frequency-domain data segments")
segments = {ifo : strain_segments_dict[ifo].fourier_segments()
           for ifo in ifos}
del strain_segments_dict

flen = len(segments[ifos[0]][0])
tlen = (flen - 1) * 2
delta_f = segments[ifos[0]][0].delta_f
psd.associate_psds_to_multi_ifo_segments(args, segments, strain_dict, flen,
        delta_f, flow, ifos, dyn_range_factor=DYN_RANGE_FAC,
        precision='single')

logging.info("Initializing signal-based vetoes.")
power_chisq = vetoes.SingleDetPowerChisq(args.chisq_bins)

logging.info("Overwhitening frequency-domain data segments")
for ifo in ifos:
    for seg in segments[ifo]:
        seg /= seg.psd
        
logging.info("Shifting data to geocentric coordinates")
for ifo in ifos:
    for j, seg in enumerate(segments[ifo]):
        det = detector.Detector(ifo)
        start = seg.start_time
        end =(seg.end_time)
        tcenter = (float(start) + float(end)) / 2.0
        dt = det.time_delay_from_earth_center(args.right_ascension, 
                                              args.declination, tcenter)
        seg[:] = seg.cyclic_time_shift(-dt)
        segments[ifo][j] = seg

logging.info("Read in template bank")
template_mem = zeros(tlen, dtype=complex64)
bank = waveform.FilterBank(args.bank_file, flen, delta_f, complex64,
                           low_frequency_cutoff=flow,
                           out=template_mem)
               
logging.info("Setting up marginalization points")
numpy.random.seed(args.marg_seed)

# points should be drawn according to prior (we'll integrate over these later)
polm = numpy.random.uniform(0, numpy.pi*2.0, size=args.marg_size)
incm = numpy.arccos(numpy.random.uniform(-1, 1, size=args.marg_size))
distm = numpy.random.uniform(350, 2000, size=args.marg_size)

htf = {}
for ifo in ifos:
    det = detector.Detector(ifo)
    fp, fc = det.antenna_pattern(args.right_ascension, 
                                  args.declination,
                                  polm,
                                  args.gps_start_time[ifos[0]])  # Reference time, might want to recalculate this....    
    ip = numpy.cos(incm)
    ic = 0.5 * (1.0 + ip * ip)
    htf[ifo] = (fp * ip + 1.0j * fc * ic) / distm
                 
def marginal_likelihood(sh, hh):
    """ Approximate marginal likelihood over pol/inc/dist. Maximized over phase.
    """
    shloglr = numpy.zeros(args.marg_size, dtype=numpy.complex128)
    hhloglr = numpy.zeros(args.marg_size, dtype=numpy.float64)
    
    for ifo in sh:
        shloglr += sh[ifo] * htf[ifo]
        hhloglr += hh[ifo] * abs(htf[ifo]) ** 2.0

    vloglr = abs(shloglr) + hhloglr
    return scipy.special.logsumexp(vloglr) - numpy.log(args.marg_size)

def threshold(vec, value, window):
    """ Pick peaks within window above threshold """
    window_idx = int(window * vec.sample_rate)
    vec1 = vec.numpy()
    nblocks = int(numpy.ceil(len(vec) / window_idx))
    nidx = nblocks * window_idx
    vec1.resize(nidx) # should add zeros to end
    vec2 = vec1.reshape(nblocks, window_idx)
    maxs = vec2.argmax(axis=1) + numpy.arange(0, nblocks) * window_idx
    return maxs[vec1[maxs] > value]

events = {}
for k in ['marg', 'template_id']:
    events[k] = []
for k in ['snr', 'chisq']:
    for ifo in ifos:
        events[ifo + '_' + k] = []

for t_num, template in enumerate(bank):
    for s_num, stilde in enumerate(segments[ifos[0]]):
        stilde = {ifo : segments[ifo][s_num] for ifo in ifos}
        logging.info('Template {}, Segment {}'.format(t_num, s_num))
        
        # calculate SNR fo reach detector
        snrs, corr, norm = {}, {}, {}
        for ifo in ifos:
            snr, corr[ifo], norm[ifo] = filter.matched_filter_core(template, stilde[ifo],
                                               h_norm=template.sigmasq(stilde[ifo].psd))
            snr *= norm[ifo]
            snr = snr[stilde[ifo].analyze]
            snrs[ifo] = snr.copy()
            logging.info('Found max peak: %s: %s', ifo, abs(snr).abs_max_loc())
            
        # Cluster and threshold triggers based on the network SNR
        net_snr = 0
        for ifo in snrs:
            net_snr += abs(snrs[ifo]) ** 2.0
        net_snr = net_snr ** 0.5
        peaks = threshold(net_snr, args.network_snr_threshold, args.cluster_window)
        logging.info('Clustered peak network SNRS: %s', net_snr[peaks])

        # Calculate signal-consistency tests and toss out triggers 
        chisq = []
        if len(peaks) > 0:
            for ifo in ifos:
                chisq, dof = power_chisq.values(corr[ifo],
                            snrs[ifo].numpy()[peaks] / norm[ifo],
                            norm[ifo], stilde[ifo].psd,
                            peaks + stilde[ifo].analyze.start, template)
                chisq /= dof
                logging.info('Chisq test: %s: %s', ifo, chisq)
        
        # Calculate the marginal likelihood approximation for each trigger
        # marginized over pol, inc, dist
        hh = {ifo: template.sigmasq(stilde[ifo].psd) * -0.5 for ifo in ifos}
        for p, cval in zip(peaks, chisq):
            sh = {ifo: snr[p] / norm[ifo] * 4 * delta_f for ifo in ifos}
            ml = marginal_likelihood(sh, hh)
            logging.info('Marginal Lik: %s %s', p, ml)

            events['marg'].append(ml)
            for ifo in ifos:
                snrp = snrs[ifo][p]
                events[ifo + '_' + 'snr'].append(snrp)
                events[ifo + '_' + 'chisq'].append(cval)

f = h5py.File(args.output_file, 'w')
for k in events:
    f[k] = numpy.concatenate(events[k])            
            
logging.info('Done')
