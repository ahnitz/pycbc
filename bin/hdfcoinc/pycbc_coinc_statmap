#!/bin/env  python
"""
The program combines coincident output files generated
by pycbc_coinc_findtrigs to generated a mapping between SNR and FAP, along
with producing the combined foreground and background triggers
"""
import argparse, h5py, logging, itertools, numpy
from scipy.interpolate import interp1d  
from pycbc.events import veto, coinc
import pycbc.version, pycbc.pnutils

def apply_bank_statistic(data, name, bank_file):
    if name == 'mchirp_bin':
        bank = h5py.File(bank_file)
        b = [0, 1, 2, 3, 4]
        data = mchirp_bin(data, bank, b)           
    return data
    
def mchirp_bin(data, bank, bins):
    logging.info("applying mchirp binning")
    r = []
    mc, et = pycbc.pnutils.mass1_mass2_to_mchirp_eta(bank['mass1'][:], bank['mass2'][:])
    for i in range(len(bins)-1):
        loc = numpy.where(numpy.logical_and(mc > bins[i], mc <= bins[i+1]))[0]
        d = data.select(numpy.in1d(data.template_id, loc))
        if len(d) > 0:
            d = d.cluster()
            fan = calculate_fan_map(d.stat, d.decimation_factor)(d.stat)
            d.data['stat'] = 1.0 / fan
        r.append(ArrayData(d.interval, d.window, data=d.data))
    return numpy.sum(r)            

class ArrayData(object):
    def __init__(self, interval, window, data=None, files=None, groups=None):
        self.data = data
        self.interval = interval
        self.window = window
        
        if files:
            self.data = {}
            for g in groups:
                self.data[g] = []
            
            for f in files:
                for g in groups:
                    d = h5py.File(f)
                    if g in d:
                        self.data[g].append(d[g][:])
                    d.close()
                    
            for k in self.data:
                self.data[k] = numpy.concatenate(self.data[k])

        for k in self.data:
            setattr(self, k, self.data[k])

    def __len__(self):
        return len(self.data[self.data.keys()[0]])

    def __add__(self, other):
        data = {}
        for k in self.data:
            data[k] = numpy.concatenate([self.data[k], other.data[k]])
        return ArrayData(self.interval, self.window, data=data)

    def select(self, idx):
        data = {}
        for k in self.data:
            data[k] = self.data[k][idx]
        return ArrayData(self.interval, self.window, data=data)
   
    def remove(self, idx):
        data = {}
        for k in self.data:
            data[k] = numpy.delete(self.data[k], idx)
        return ArrayData(self.interval, self.window, data=data)

    def cluster(self):
        cid = coinc.cluster_coincs(self.stat, self.time1, self.time2,
                                 self.timeslide_id, self.interval, self.window)
        return self.select(cid) 
         
def load_coincs(coinc_files, window):
    columns = ['stat', 'time1', 'time2', 'trigger_id1', 'trigger_id2', 
               'template_id', 'decimation_factor', 'timeslide_id']
    f = h5py.File(coinc_files[0])
    d = ArrayData(f.attrs['timeslide_interval'], window, files=coinc_files, groups=columns)
    return (d, dict(f.attrs), f['segments'])
           
def calculate_fan_map(combined_stat, dec):
    """ Return a function to map between false alarm number (FAN) and the
    combined ranking statistic.
    """
    stat_sorting = combined_stat.argsort()    
    combined_stat = combined_stat[stat_sorting]
    fan = dec[stat_sorting][::-1].cumsum()[::-1]    
    return interp1d(combined_stat, fan, fill_value=1, bounds_error=False) 

def sec_to_year(sec):
    return sec / (3.15569e7)

parser = argparse.ArgumentParser()
# General required options
parser.add_argument('--version', action='version', 
         version=pycbc.version.git_verbose_msg)
parser.add_argument('--coinc-files', nargs='+', 
         help='List of coincidence files used to calculate the FAP, FAR, etc.')
parser.add_argument('--verbose', action='count')
parser.add_argument('--cluster-window', type=float, 
         help='Size in seconds to maximize coinc triggers')
parser.add_argument('--veto-window', type=float, 
         help='window around each zerolag trigger to window out')
parser.add_argument('--cross-bank-statistic', default=None)
parser.add_argument('--output-file')
parser.add_argument('--bank-file')
args = parser.parse_args()
pycbc.init_logging(args.verbose)

logging.info("Loading coinc triggers")    
d, attrs, seg = load_coincs(args.coinc_files, args.cluster_window)   
logging.info("We have %s triggers" % len(d.stat))

logging.info("Clustering coinc triggers (inclusive of zerolag)")
d = apply_bank_statistic(d, args.cross_bank_statistic, args.bank_file).cluster()

fore_locs = d.timeslide_id == 0
logging.info("%s clustered foreground triggers" % fore_locs.sum())

ft1, ft2 = d.time1[fore_locs], d.time2[fore_locs]
vt = (ft1 + ft2) / 2.0
veto_start, veto_end = vt - args.veto_window, vt + args.veto_window
veto_time = abs(veto.start_end_to_segments(veto_start, veto_end).coalesce())  
v1 = veto.indices_within_times(d.time1, veto_start, veto_end) 
v2 = veto.indices_within_times(d.time2, veto_start, veto_end) 
e = d.remove(v1).remove(v2)
logging.info("Clustering coinc triggers (exclusive of zerolag)")
e = apply_bank_statistic(e, args.cross_bank_statistic, args.bank_file).cluster()

logging.info("Dumping foreground triggers")
f = h5py.File(args.output_file, "w")
f.attrs['detector_1'] = attrs['detector_1']
f.attrs['detector_2'] = attrs['detector_2']
f.attrs['timeslide_interval'] = attrs['timeslide_interval']

# Copy over the segment for coincs and singles
for key in seg.keys():
    f['segments/%s/start' % key] = seg[key]['start'][:]
    f['segments/%s/end' % key] = seg[key]['end'][:]

f['segments/foreground_veto/start'] = veto_start
f['segments/foreground_veto/end'] = veto_end

if fore_locs.sum() > 0:
    for k in d.data:
        f['foreground/' + k] = d.data[k][fore_locs]



back_locs = d.timeslide_id != 0

if (back_locs.sum()) == 0:
    logging.warn("There were no background events, so we could not assign "
                 "any statistic values")
    exit()
    
logging.info("Dumping background triggers (inclusive of zerolag)")
for k in d.data:
    f['background/' + k] = d.data[k][back_locs]
    
logging.info("Dumping background triggers (exclusive of zerolag)")   
for k in e.data:
    f['background_exc/' + k] = e.data[k]

maxtime = max(attrs['foreground_time1'], attrs['foreground_time2'])
mintime = min(attrs['foreground_time1'], attrs['foreground_time2'])

maxtime_exc = maxtime - veto_time
mintime_exc = mintime - veto_time

background_time = int(maxtime / attrs['timeslide_interval']) * mintime
coinc_time = float(attrs['coinc_time'])

background_time_exc = int(maxtime_exc / attrs['timeslide_interval']) * mintime_exc
coinc_time_exc = coinc_time - veto_time

logging.info("Making mapping from FAN to the combined statistic")
back_stat = d.stat[back_locs]
fanmap = calculate_fan_map(back_stat, d.decimation_factor[back_locs])       
back_fan = fanmap(back_stat)

fanmap_exc = calculate_fan_map(e.stat, e.decimation_factor)     
back_fan_exc = fanmap_exc(e.stat)         

f['background/fan'] = back_fan
f['background/ifar'] = sec_to_year(background_time / back_fan)  
f['background_exc/fan'] = back_fan_exc
f['background_exc/ifar'] = sec_to_year(background_time_exc / back_fan_exc)

f.attrs['background_time'] = background_time
f.attrs['foreground_time'] = coinc_time
f.attrs['background_time_exc'] = background_time_exc
f.attrs['foreground_time_exc'] = coinc_time_exc

logging.info("calculating ifar values")
fore_stat = d.stat[fore_locs]

fore_fan = fanmap(fore_stat)
ifar = background_time / fore_fan

fore_fan_exc = fanmap_exc(fore_stat)
ifar_exc = background_time_exc / fore_fan_exc

logging.info("calculating fap values")
fap = numpy.clip(coinc_time/ifar, 0, 1)
fap_exc = numpy.clip(coinc_time_exc/ifar_exc, 0, 1)
if fore_locs.sum() > 0:
    f['foreground/fan'] = fore_fan
    f['foreground/ifar'] = sec_to_year(ifar)
    f['foreground/fap'] = fap
    
    f['foreground/fan_exc'] = fore_fan_exc
    f['foreground/ifar_exc'] = sec_to_year(ifar_exc)
    f['foreground/fap_exc'] = fap_exc

logging.info("Done") 
